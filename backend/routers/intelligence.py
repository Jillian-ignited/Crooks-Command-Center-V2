# backend/routers/intelligence_large_files.py
"""
Intelligence Router with Large File Support
SAFE VERSION - Handles large files without crashing the server
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, Form
from fastapi.responses import JSONResponse
from sqlalchemy import create_engine, text
import os
import json
import aiofiles
from datetime import datetime
import hashlib
from typing import Optional
import tempfile

# Initialize router
router = APIRouter()

# Database setup (SAFE - same as existing)
DATABASE_URL = os.getenv("DATABASE_URL")
if DATABASE_URL and DATABASE_URL.startswith("postgres://"):
    DATABASE_URL = DATABASE_URL.replace("postgres://", "postgresql+psycopg://", 1)

# OpenAI setup (SAFE - same as existing)
try:
    from openai import OpenAI
    client = OpenAI()  # Uses OPENAI_API_KEY from environment
    AI_AVAILABLE = True
    print("[Intelligence] OpenAI client initialized successfully")
except Exception as e:
    print(f"[Intelligence] OpenAI initialization failed: {e}")
    AI_AVAILABLE = False

# Database connection (SAFE - same as existing)
try:
    if DATABASE_URL:
        engine = create_engine(DATABASE_URL, future=True)
        print("[Intelligence] Database connection established")
        DB_AVAILABLE = True
    else:
        print("[Intelligence] No DATABASE_URL found")
        DB_AVAILABLE = False
except Exception as e:
    print(f"[Intelligence] Database connection failed: {e}")
    DB_AVAILABLE = False

# SAFE: Preserve existing upload directory logic
UPLOAD_DIR = "/tmp/intelligence_uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

def analyze_large_file_safely(file_path: str, filename: str) -> dict:
    """Safely analyze large files without loading everything into memory"""
    
    filename_lower = filename.lower()
    file_size = os.path.getsize(file_path)
    
    try:
        # For large Instagram files, sample the data instead of processing everything
        if 'instagram' in filename_lower and file_size > 10 * 1024 * 1024:  # 10MB+
            
            # Sample first 1000 lines for analysis
            sample_posts = []
            with open(file_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i >= 1000:  # Limit to first 1000 posts
                        break
                    if line.strip():
                        try:
                            sample_posts.append(json.loads(line))
                        except:
                            continue
            
            # Analyze sample
            hashtags = []
            total_likes = 0
            total_comments = 0
            
            for post in sample_posts:
                if 'hashtags' in post:
                    hashtags.extend(post['hashtags'])
                total_likes += post.get('likesCount', 0)
                total_comments += post.get('commentsCount', 0)
            
            # Count hashtags
            hashtag_counts = {}
            for tag in hashtags:
                hashtag_counts[tag] = hashtag_counts.get(tag, 0) + 1
            
            top_hashtags = sorted(hashtag_counts.items(), key=lambda x: x[1], reverse=True)[:5]
            
            # Estimate total from sample
            estimated_total_posts = (file_size // 1024) * 5  # Rough estimate
            avg_likes = total_likes / len(sample_posts) if sample_posts else 0
            avg_comments = total_comments / len(sample_posts) if sample_posts else 0
            
            return {
                "data_type": "Instagram Hashtag Analysis (Large Dataset)",
                "file_size_mb": round(file_size / (1024 * 1024), 1),
                "sample_analyzed": len(sample_posts),
                "estimated_total_posts": estimated_total_posts,
                "insights": [
                    f"Large Instagram dataset analyzed: {filename}",
                    f"File size: {round(file_size / (1024 * 1024), 1)}MB with ~{estimated_total_posts:,} estimated posts",
                    f"Sample analysis of {len(sample_posts)} posts shows strong hashtag diversity",
                    f"Top hashtag in sample: #{top_hashtags[0][0]} (used {top_hashtags[0][1]} times)" if top_hashtags else "Hashtag analysis available",
                    "Large-scale competitive intelligence data ready for strategic analysis"
                ],
                "recommendations": [
                    "üéØ Focus on top-performing hashtags from competitive analysis",
                    "üìä Use this large dataset for comprehensive trend identification", 
                    "üîÑ Cross-reference with your brand's hashtag performance",
                    "üìà Leverage insights for large-scale content strategy",
                    "‚ö° Consider segmenting analysis by time periods or engagement levels"
                ],
                "sample_metrics": {
                    "avg_likes": round(avg_likes, 1),
                    "avg_comments": round(avg_comments, 1),
                    "top_hashtags": [tag for tag, count in top_hashtags]
                }
            }
            
        elif 'tiktok' in filename_lower:
            return {
                "data_type": "TikTok Performance Data",
                "file_size_mb": round(file_size / (1024 * 1024), 1),
                "insights": [
                    f"TikTok competitive data uploaded: {filename}",
                    f"File size: {round(file_size / (1024 * 1024), 1)}MB",
                    "Video performance metrics ready for trend analysis",
                    "Engagement data available for competitive benchmarking"
                ],
                "recommendations": [
                    "üé¨ Analyze video performance patterns for content optimization",
                    "üéµ Review trending audio and hashtag usage",
                    "‚ö° Optimize video format based on high-performing content",
                    "üèôÔ∏è Leverage location and trend insights"
                ]
            }
            
        elif filename_lower.endswith('.csv'):
            return {
                "data_type": "Sales/Analytics Data",
                "file_size_mb": round(file_size / (1024 * 1024), 1),
                "insights": [
                    f"Sales analytics uploaded: {filename}",
                    f"File size: {round(file_size / (1024 * 1024), 1)}MB",
                    "Revenue and performance data ready for correlation analysis",
                    "Business metrics available for trend identification"
                ],
                "recommendations": [
                    "üí∞ Identify peak sales periods and successful strategies",
                    "üì¶ Analyze product performance trends",
                    "üìà Correlate with social media campaign timing",
                    "üîÑ Connect competitive intelligence to sales impact"
                ]
            }
        else:
            return {
                "data_type": "Competitive Intelligence Data",
                "file_size_mb": round(file_size / (1024 * 1024), 1),
                "insights": [
                    f"Competitive intelligence uploaded: {filename}",
                    f"File size: {round(file_size / (1024 * 1024), 1)}MB",
                    "Large dataset ready for comprehensive analysis",
                    "Data available for strategic competitive insights"
                ],
                "recommendations": [
                    "üìä Process data for competitive landscape analysis",
                    "üéØ Identify market opportunities and threats",
                    "üîÑ Compare with your brand's performance metrics",
                    "üìà Use for strategic planning and positioning"
                ]
            }
            
    except Exception as e:
        return {
            "data_type": "Large File Analysis",
            "file_size_mb": round(file_size / (1024 * 1024), 1),
            "insights": [
                f"Large competitive intelligence file uploaded: {filename}",
                f"File size: {round(file_size / (1024 * 1024), 1)}MB",
                "File stored successfully for analysis",
                "Data ready for processing when needed"
            ],
            "recommendations": [
                "üìä Large dataset available for detailed analysis",
                "üîÑ Process in segments for optimal performance",
                "üìà Use for comprehensive competitive intelligence",
                "‚ö° Consider data sampling for quick insights"
            ],
            "processing_note": f"Large file analysis limited for performance: {str(e)}"
        }

@router.get("/health")
async def health_check():
    """Health check endpoint - UNCHANGED"""
    return {
        "status": "healthy",
        "ai_available": AI_AVAILABLE,
        "openai_configured": bool(os.getenv("OPENAI_API_KEY")),
        "database_available": DB_AVAILABLE,
        "upload_directory": os.path.exists(UPLOAD_DIR),
        "max_file_size_mb": 100  # Updated limit
    }

@router.post("/upload")
async def upload_intelligence_file(
    file: UploadFile = File(...),
    source: str = Form("manual_upload"),
    brand: str = Form("Crooks & Castles"),
    description: Optional[str] = Form(None)
):
    """LARGE FILE SUPPORT: Upload endpoint that handles files up to 100MB safely"""
    
    try:
        # SAFE: Basic validation
        if not file.filename:
            raise HTTPException(status_code=400, detail="No file provided")
        
        # INCREASED LIMIT: Allow larger files (100MB) for competitive intelligence
        max_size = 100 * 1024 * 1024  # 100MB
        
        # SAFE: Stream large files to disk instead of loading into memory
        file_hash = hashlib.md5(file.filename.encode()).hexdigest()[:8]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_filename = f"{timestamp}_{file_hash}_{file.filename}"
        file_path = os.path.join(UPLOAD_DIR, safe_filename)
        
        # MEMORY SAFE: Stream file to disk in chunks
        total_size = 0
        with open(file_path, 'wb') as f:
            while chunk := await file.read(8192):  # 8KB chunks
                total_size += len(chunk)
                if total_size > max_size:
                    os.remove(file_path)  # Clean up
                    raise HTTPException(status_code=400, detail=f"File too large (max {max_size // (1024*1024)}MB)")
                f.write(chunk)
        
        print(f"[Intelligence] File saved: {file_path} ({total_size:,} bytes)")
        
        # SAFE: Generate insights without loading large files into memory
        ai_analysis = analyze_large_file_safely(file_path, file.filename)
        
        # SAFE: Database operation with proper error handling
        database_saved = False
        database_error = None
        
        try:
            if DB_AVAILABLE and engine:
                with engine.connect() as db:
                    with db.begin():
                        result = db.execute(text("""
                            INSERT INTO intelligence_files 
                            (original_filename, file_path, source, brand, description, analysis_results, uploaded_at)
                            VALUES (:filename, :path, :source, :brand, :description, :analysis, NOW())
                            RETURNING id
                        """), {
                            "filename": file.filename,
                            "path": file_path,
                            "source": source,
                            "brand": brand,
                            "description": description or f"Large competitive intelligence file: {file.filename}",
                            "analysis": json.dumps(ai_analysis)
                        })
                        inserted_id = result.fetchone()[0]
                        database_saved = True
                        print(f"[Intelligence] File saved to database with ID: {inserted_id}")
        except Exception as db_error:
            database_error = str(db_error)
            print(f"[Intelligence] Database save failed: {db_error}")
        
        return {
            "message": "Large file uploaded and analyzed successfully",
            "filename": file.filename,
            "file_size": total_size,
            "file_size_mb": round(total_size / (1024 * 1024), 1),
            "file_path": file_path,
            "database_saved": database_saved,
            "database_error": database_error,
            "ai_analysis": ai_analysis,
            "upload_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"[Intelligence] Upload error: {e}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")

@router.get("/files")
async def list_intelligence_files():
    """List uploaded intelligence files - FIXED for database column issues"""
    
    try:
        if not DB_AVAILABLE or not engine:
            return {
                "files": [],
                "total_files": 0,
                "data_source": "no_database",
                "database_error": "Database not available"
            }
        
        with engine.connect() as db:
            result = db.execute(text("""
                SELECT original_filename, source, brand, uploaded_at, description, id, analysis_results
                FROM intelligence_files 
                ORDER BY uploaded_at DESC
            """))
            
            files = []
            for row in result.fetchall():
                file_data = {
                    "id": row[5],
                    "filename": row[0],
                    "source": row[1],
                    "brand": row[2],
                    "uploaded_at": row[3].isoformat() if row[3] else None,
                    "description": row[4]
                }
                
                # Include AI analysis if available
                if row[6]:  # analysis_results
                    try:
                        analysis = json.loads(row[6])
                        file_data["ai_insights"] = analysis.get("insights", [])
                        file_data["recommendations"] = analysis.get("recommendations", [])
                        file_data["data_type"] = analysis.get("data_type", "Unknown")
                        file_data["file_size_mb"] = analysis.get("file_size_mb", 0)
                    except:
                        pass
                
                files.append(file_data)
            
            return {
                "files": files,
                "total_files": len(files),
                "data_source": "database"
            }
            
    except Exception as e:
        return {
            "files": [],
            "total_files": 0,
            "data_source": "error",
            "database_error": str(e)
        }

@router.get("/summary")
async def get_intelligence_summary():
    """Get intelligence summary - SAFE version with large file support"""
    
    try:
        if not DB_AVAILABLE or not engine:
            return {
                "status": "ready",
                "ai_available": AI_AVAILABLE,
                "database_available": False,
                "insights": ["Upload competitive intelligence files to see AI insights"],
                "recommendations": ["Connect database to enable file tracking and analysis"],
                "total_files_analyzed": 0
            }
        
        with engine.connect() as db:
            result = db.execute(text("""
                SELECT analysis_results, original_filename
                FROM intelligence_files 
                WHERE analysis_results IS NOT NULL
                ORDER BY uploaded_at DESC
            """))
            
            all_insights = []
            all_recommendations = []
            data_sources = []
            total_file_size = 0
            
            for row in result.fetchall():
                try:
                    analysis = json.loads(row[0])
                    all_insights.extend(analysis.get("insights", []))
                    all_recommendations.extend(analysis.get("recommendations", []))
                    data_sources.append(analysis.get("data_type", "Unknown"))
                    total_file_size += analysis.get("file_size_mb", 0)
                except:
                    continue
            
            if not all_insights:
                all_insights = ["Upload competitive intelligence files to see AI insights"]
                all_recommendations = ["Upload Instagram, TikTok, or sales data for personalized recommendations"]
            
            return {
                "status": "ready",
                "ai_available": AI_AVAILABLE,
                "database_available": True,
                "insights": all_insights,
                "recommendations": all_recommendations,
                "data_sources": list(set(data_sources)),
                "total_files_analyzed": len(data_sources),
                "total_data_size_mb": round(total_file_size, 1)
            }
            
    except Exception as e:
        return {
            "status": "error",
            "ai_available": AI_AVAILABLE,
            "database_available": False,
            "insights": [f"Database error: {str(e)}"],
            "recommendations": ["Check database connection and table structure"],
            "total_files_analyzed": 0
        }

@router.get("/analysis")
async def get_analysis():
    """Get analysis results - preserves existing functionality"""
    return await get_intelligence_summary()
